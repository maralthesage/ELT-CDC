{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ab2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dbfread import DBF, DBFNotFound\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c68f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_dbf_in_chunks(file_path, chunk_size):\n",
    "    \"\"\"\n",
    "    Read a .dbf file in chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        table = DBF(file_path, encoding='cp850', ignore_missing_memofile=True)\n",
    "        chunk = []\n",
    "        for record in table:\n",
    "            chunk.append(record)\n",
    "            if len(chunk) == chunk_size:\n",
    "                yield pd.DataFrame(chunk)\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            yield pd.DataFrame(chunk)\n",
    "    except DBFNotFound:\n",
    "        print(f\"DBF file {file_path} not found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_csv_in_chunks(chunks, file_name, append=False):\n",
    "    \"\"\"\n",
    "    Write chunks of data to a CSV file incrementally.\n",
    "    \"\"\"\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        mode = 'a' if append or i > 0 else 'w'\n",
    "        header = not (append or i > 0)\n",
    "        chunk.to_csv(file_name, mode=mode, header=header, index=False, sep=';', encoding='cp850')\n",
    "        print(f\"Chunk {i + 1} written to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3fc5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def etl_flow(chunk_size=1000):\n",
    "    \"\"\"\n",
    "    ETL flow for processing .dbf files and saving them as .csv.\n",
    "    \"\"\"\n",
    "    lands = ['F01','F02', 'F03', 'F04']\n",
    "    \n",
    "    files = [\n",
    "        'V2AD1001', 'V2AD1056', 'V2AD1096', 'V2AD1156', 'V2AD1004', 'V2AD1005',  \n",
    "        'V2AR1001', 'V2AR1002', 'V2AR1004', 'V2AR1005', 'V2AR1007', \n",
    "        'V2LA1001', 'V2LA1002', 'V2LA1003', 'V2LA1005', 'V2LA1006', 'V2LA1008', \n",
    "        'V4AR1009', 'V2AD1009', 'V4LA1009',\n",
    "    ]\n",
    "    ## Werbehistorie ==> 'V4AD1023'\n",
    "    ## Nachfrage tabelle ==> 'V2SC1010',\n",
    "    ## Kundenwanderung  ==> 'V2AD2000',\n",
    "\n",
    "    files = ['V2SC1010']\n",
    "    # lands = ['F01']\n",
    "\n",
    "    total_files = len(lands) * len(files)\n",
    "    processed_files = 0\n",
    "    progress_bar = tqdm(total=total_files, desc=\"Processing files\", ncols=100, dynamic_ncols=True)\n",
    "\n",
    "    for LAND in lands:\n",
    "        for FILE_NAME in files:\n",
    "            processed_files += 1\n",
    "            dbf_file_path = fr'/Volumes/DATA/{LAND}/{FILE_NAME}.dbf'\n",
    "            csv_file_path = fr'/Volumes/MARAL/CSV/{LAND}/{FILE_NAME}.csv'\n",
    "\n",
    "            print(f\"\\n[{datetime.now()}] Processing file {processed_files}/{total_files}: {FILE_NAME} ({LAND})\")\n",
    "\n",
    "            if os.path.exists(csv_file_path):\n",
    "                modification_time = os.path.getmtime(csv_file_path)\n",
    "                modification_date = datetime.fromtimestamp(modification_time).date()\n",
    "                if modification_date == datetime.today().date():\n",
    "                    print(f\"File {csv_file_path} is up-to-date.\")\n",
    "                    continue\n",
    "\n",
    "            chunks = read_dbf_in_chunks(dbf_file_path, chunk_size)\n",
    "            write_csv_in_chunks(chunks, csv_file_path)\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac1571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    etl_flow(chunk_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054289a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4208e7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
